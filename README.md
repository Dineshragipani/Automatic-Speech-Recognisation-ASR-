# Automatic-Speech-Recognisation-ASR-
Automatic speech recognisation(ASR)
It is a AI model generated by using hugging face transformers. It has 3 phases.
1.Resampling the audio data
2.Filtering the dataset
3.Converting audio data to the modelâ€™s expected input
RESAMPLIING THE AUDIO DATA:
The load_dataset function downloads audio examples with the sampling rate that they were published with. This is not always the sampling rate expected by a model you plan to train, or use for inference. If thereâ€™s a discrepancy between the sampling rates, you can resample the audio to the modelâ€™s expected sampling rate.
Most of the available pretrained models have been pretrained on audio datasets at a sampling rate of 16 kHz. When we explored MINDS-14 dataset, you may have noticed that it is sampled at 8 kHz, which means we will likely need to upsample it.
To do so, use ðŸ¤— Datasetsâ€™ cast_column method. This operation does not change the audio in-place, but rather signals to datasets to resample the audio examples on the fly when they are loaded. The following code will set the sampling rate to 16kHz.
Re-load the first audio example in the MINDS-14 dataset, and check that it has been resampled to the desired sampling rate.
Some background on resampling: If an audio signal has been sampled at 8 kHz, so that it has 8000 sample readings per second, we know that the audio does not contain any frequencies over 4 kHz. This is guaranteed by the Nyquist sampling theorem. Because of this, we can be certain that in between the sampling points the original continuous signal always makes a smooth curve. Upsampling to a higher sampling rate is then a matter of calculating additional sample values that go in between the existing ones, by approximating this curve. Downsampling, however, requires that we first filter out any frequencies that would be higher than the new Nyquist limit, before estimating the new sample points. In other words, you can't downsample by a factor 2x by simply throwing away every other sample â€” this will create distortions in the signal called aliases. Doing resampling correctly is tricky and best left to well-tested libraries such as librosa or ðŸ¤— Datasets.
FILTERIING THE DATASET:
You may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a certain duration. For instance, we might want to filter out any examples longer than 20s to prevent out-of-memory errors when training a model.
We can do this by using the ðŸ¤— Datasetsâ€™ filter method and passing a function with filtering logic to it. Letâ€™s start by writing a function that indicates which examples to keep and which to discard. This function, is_audio_length_in_range, returns True if a sample is shorter than 20s, and False if it is longer than 20s.
The filtering function can be applied to a datasetâ€™s column but we do not have a column with audio track duration in this dataset. However, we can create one, filter based on the values in that column, and then remove it.
One of the most challenging aspects of working with audio datasets is preparing the data in the right format for model training. As you saw, the raw audio data comes as an array of sample values. However, pre-trained models, whether you use them for inference, or want to fine-tune them for your task, expect the raw data to be converted into input features. The requirements for the input features may vary from one model to another â€” they depend on the modelâ€™s architecture, and the data it was pre-trained with. The good news is, for every supported audio model, ðŸ¤— Transformers offer a feature extractor class that can convert raw audio data into the input features the model expects.
So what does a feature extractor do with the raw audio data? Letâ€™s take a look at Whisperâ€™s feature extractor to understand some common feature extraction transformations. Whisper is a pre-trained model for automatic speech recognition (ASR) published in September 2022 by Alec Radford et al. from OpenAI.
First, the Whisper feature extractor pads/truncates a batch of audio examples such that all examples have an input length of 30s. Examples shorter than this are padded to 30s by appending zeros to the end of the sequence (zeros in an audio signal correspond to no signal or silence). Examples longer than 30s are truncated to 30s. Since all elements in the batch are padded/truncated to a maximum length in the input space, there is no need for an attention mask. Whisper is unique in this regard, most other audio models require an attention mask that details where sequences have been padded, and thus where they should be ignored in the self-attention mechanism. Whisper is trained to operate without an attention mask and infer directly from the speech signals where to ignore the inputs.
The second operation that the Whisper feature extractor performs is converting the padded audio arrays to log-mel spectrograms. As you recall, these spectrograms describe how the frequencies of a signal change over time, expressed on the mel scale and measured in decibels (the log part) to make the frequencies and amplitudes more representative of human hearing.
All these transformations can be applied to your raw audio data with a couple of lines of code. Letâ€™s go ahead and load the feature extractor from the pre-trained Whisper checkpoint to have ready for our audio data.
Now you can see what the audio input to the Whisper model looks like after preprocessing.
The modelâ€™s feature extractor class takes care of transforming raw audio data to the format that the model expects. However, many tasks involving audio are multimodal, e.g. speech recognition. In such cases ðŸ¤— Transformers also offer model-specific tokenizers to process the text inputs. For a deep dive into tokenizers, please refer to our NLP course.
You can load the feature extractor and tokenizer for Whisper and other multimodal models separately, or you can load both via a so-called processor. To make things even simpler, use AutoProcessor to load a modelâ€™s feature extractor and processor from a checkpoint.
Here we have illustrated the fundamental data preparation steps. Of course, custom data may require more complex preprocessing. In this case, you can extend the function prepare_dataset to perform any sort of custom data transformations. With ðŸ¤— Datasets, if you can write it as a Python function, you can apply it to your dataset.
1. STREAMING THE AUDIO
One of the biggest challenges faced with audio datasets is their sheer size. A single minute of uncompressed CD-quality audio (44.1kHz, 16-bit) takes up a bit more than 5 MB of storage. Typically, an audio dataset would contains hours of recordings.
In the previous sections we used a very small subset of MINDS-14 audio dataset, however, typical audio datasets are much larger. For example, the xs (smallest) configuration of GigaSpeech from SpeechColab contains only 10 hours of training data, but takes over 13GB of storage space for download and preparation. So what happens when we want to train on a larger split? The full xl configuration of the same dataset contains 10,000 hours of training data, requiring over 1TB of storage space. For most of us, this well exceeds the specifications of a typical hard drive disk. Do we need to fork out and buy additional storage? Or is there a way we can train on these datasets with no disk space constraints?
ðŸ¤— Datasets comes to the rescue by offering the streaming mode. Streaming allows us to load the data progressively as we iterate over the dataset. Rather than downloading the whole dataset at once, we load the dataset one example at a time. We iterate over the dataset, loading and preparing examples on the fly when they are needed. This way, we only ever load the examples that weâ€™re using, and not the ones that weâ€™re not! Once weâ€™re done with an example sample, we continue iterating over the dataset and load the next one.
Streaming mode has three primary advantages over downloading the entire dataset at once:
Disk space: examples are loaded to memory one-by-one as we iterate over the dataset. Since the data is not downloaded locally, there are no disk space requirements, so you can use datasets of arbitrary size.
Download and processing time: audio datasets are large and need a significant amount of time to download and process. With streaming, loading and processing is done on the fly, meaning you can start using the dataset as soon as the first example is ready.
Easy experimentation: you can experiment on a handful of examples to check that your script works without having to download the entire dataset.
There is one caveat to streaming mode. When downloading a full dataset without streaming, both the raw data and processed data are saved locally to disk. If we want to re-use this dataset, we can directly load the processed data from disk, skipping the download and processing steps. Consequently, we only have to perform the downloading and processing operations once, after which we can re-use the prepared data.
With streaming mode, the data is not downloaded to disk. Thus, neither the downloaded nor pre-processed data are cached. If we want to re-use the dataset, the streaming steps must be repeated, with the audio files loaded and processed on the fly again. For this reason, it is advised to download datasets that you are likely to use multiple times.
Just like we applied preprocessing steps to a downloaded subset of MINDS-14, you can do the same preprocessing with a streaming dataset in the exactly the same manner.
The only difference is that you can no longer access individual samples using Python indexing (i.e. gigaspeech["train"][sample_idx]). Instead, you have to iterate over the dataset. Hereâ€™s how you can access an example when streaming a dataset.
If youâ€™d like to preview several examples from a large dataset, use the take() to get the first n elements. Letâ€™s grab the first two examples in the gigaspeech dataset.
Streaming mode can take your research to the next level: not only are the biggest datasets accessible to you, but you can easily evaluate systems over multiple datasets in one go without worrying about your disk space. Compared to evaluating on a single dataset, multi-dataset evaluation gives a better metric for the generalisation abilities of a speech recognition system (c.f. End-to-end Speech Benchmark (ESB)).
